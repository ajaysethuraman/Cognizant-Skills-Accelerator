# -*- coding: utf-8 -*-
"""Untitled30.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dXvWNYLvrxzMsCW6dogDaKv-tAASHceE
"""

pip install transformers flask torch datasets nltk pandas

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, T5ForConditionalGeneration, T5Tokenizer, BertForSequenceClassification, BertTokenizer
from transformers import Trainer, TrainingArguments
from flask import Flask, request, jsonify
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from datasets import load_dataset

nltk.download('punkt')

# Load the pre-trained models and tokenizers
chatbot_model = GPT2LMHeadModel.from_pretrained('gpt2')
chatbot_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

summarization_model = T5ForConditionalGeneration.from_pretrained('t5-small')
summarization_tokenizer = T5Tokenizer.from_pretrained('t5-small')

sentiment_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
sentiment_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Example datasets (small subset for testing)
chatbot_data = [
    ("How can I reset my password?", "To reset your password, click on 'Forgot password' at the login screen."),
    ("Where can I find my order status?", "You can find your order status in your account under 'Order History'.")
]

summarization_data = [
    ("The global climate change issue is becoming more critical each day due to the rising levels of CO2.", "Summary of the article about climate change.")
]

sentiment_data = [
    ("I love this product, it's amazing!", 1),  # 1 for positive sentiment
    ("This is the worst purchase I've made.", 0)  # 0 for negative sentiment
]

# Convert to pandas DataFrame
chatbot_df = pd.DataFrame(chatbot_data, columns=["Question", "Answer"])
summarization_df = pd.DataFrame(summarization_data, columns=["Text", "Summary"])
sentiment_df = pd.DataFrame(sentiment_data, columns=["Text", "Sentiment"])

def fine_tune_chatbot(chatbot_data):
    # Add padding token to the tokenizer
    chatbot_tokenizer.pad_token = chatbot_tokenizer.eos_token # Use eos_token as pad_token

    # Tokenize inputs and labels with padding and truncation
    encodings = chatbot_tokenizer([q[0] for q in chatbot_data],
                                   text_pair=[q[1] for q in chatbot_data],  # Tokenize input-output pairs together
                                   return_tensors="pt",
                                   padding=True,
                                   truncation=True)

    # Ensure labels have the same shape as inputs and are on the same device
    encodings["labels"] = encodings["input_ids"].clone().detach()
    encodings.to(chatbot_model.device)  # Move all tensors to the model's device

    # Set labels to -100 where padding tokens are present in the input_ids
    encodings["labels"] = torch.where(encodings["attention_mask"] == 0, -100, encodings["labels"])

    # Forward pass
    outputs = chatbot_model(**encodings)  # Pass the encodings dictionary directly
    loss = outputs.loss
    return loss

fine_tune_chatbot(chatbot_data)

def fine_tune_summarization(summarization_data):
    inputs = summarization_tokenizer(summarization_data["Text"].tolist(), return_tensors="pt", padding=True, truncation=True)
    labels = summarization_tokenizer(summarization_data["Summary"].tolist(), return_tensors="pt", padding=True, truncation=True).input_ids
    outputs = summarization_model(input_ids=inputs["input_ids"], labels=labels)
    loss = outputs.loss
    return loss

# Fine-tune the summarization model (simplified for demonstration)
fine_tune_summarization(summarization_df)

def fine_tune_sentiment_analysis(sentiment_data):
    inputs = sentiment_tokenizer(sentiment_data["Text"].tolist(), return_tensors="pt", padding=True, truncation=True)
    labels = torch.tensor(sentiment_data["Sentiment"].tolist())
    outputs = sentiment_model(input_ids=inputs["input_ids"], labels=labels)
    loss = outputs.loss
    return loss

# Fine-tune the sentiment model (simplified for demonstration)
fine_tune_sentiment_analysis(sentiment_df)

app = Flask(__name__)

# Chatbot endpoint
@app.route('/chatbot', methods=['POST'])
def chatbot():
    input_text = request.json['text']
    tokens = chatbot_tokenizer(input_text, return_tensors='pt')
    response_ids = chatbot_model.generate(tokens['input_ids'])
    response = chatbot_tokenizer.decode(response_ids[0], skip_special_tokens=True)
    return jsonify({'response': response})

# Summarization endpoint
@app.route('/summarize', methods=['POST'])
def summarize():
    input_text = request.json['text']
    inputs = summarization_tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)
    summary_ids = summarization_model.generate(inputs["input_ids"])
    summary = summarization_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return jsonify({'summary': summary})

# Sentiment analysis endpoint
@app.route('/sentiment', methods=['POST'])
def sentiment():
    input_text = request.json['text']
    inputs = sentiment_tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)
    outputs = sentiment_model(**inputs)
    sentiment_score = torch.argmax(outputs.logits, dim=1).item()
    sentiment = 'Positive' if sentiment_score == 1 else 'Negative'
    return jsonify({'sentiment': sentiment})

if __name__ == '__main__':
    app.run(debug=True)

!pip install flask flask-ngrok

from flask import Flask, request, jsonify
from flask_ngrok import run_with_ngrok  # Import ngrok for tunneling

app = Flask(__name__)
run_with_ngrok(app)  # Enables ngrok

@app.route("/", methods=["GET"])
def home():
    return "Flask app is running!"

@app.route("/chatbot", methods=["POST"])
def chatbot():
    data = request.json
    user_input = data.get("text", "")
    response = {"reply": f"You said: {user_input}"}
    return jsonify(response)

if __name__ == "__main__":
    app.run()