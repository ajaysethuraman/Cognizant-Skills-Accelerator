{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLoHCxt9Lh7V",
        "outputId": "486e0cc0-fe5b-488d-ecac-2e4b97e88686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "!pip install tensorflow numpy nltk\n",
        "import nltk\n",
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "# Choose a text (e.g., Shakespeare’s \"Hamlet\")\n",
        "text = gutenberg.raw('shakespeare-hamlet.txt').lower()\n",
        "\n",
        "# Tokenization: Convert text into characters\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {char: number for number, char in enumerate(chars)}\n",
        "int_to_char = {number: char for number, char in enumerate(chars)}\n",
        "\n",
        "# Create sequences of fixed length (e.g., 50 characters)\n",
        "seq_length = 50\n",
        "sequences = []\n",
        "next_chars = []\n",
        "for i in range(len(text) - seq_length):\n",
        "    sequences.append(text[i:i + seq_length])\n",
        "    next_chars.append(text[i + seq_length])\n",
        "\n",
        "# Convert sequences to integer indices\n",
        "X = np.zeros((len(sequences), seq_length, len(chars)), dtype=np.bool_)\n",
        "y = np.zeros((len(sequences), len(chars)), dtype=np.bool_)\n",
        "for i, seq in enumerate(sequences):\n",
        "    for j, char in enumerate(seq):\n",
        "        X[i, j, char_to_int[char]] = 1\n",
        "    y[i, char_to_int[next_chars[i]]] = 1"
      ],
      "metadata": {
        "id": "0PbAJfSyfuoj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "# Change 'lr' to 'learning_rate'\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))"
      ],
      "metadata": {
        "id": "M9wJci7lf8sx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('text_generation_lstm.h5', save_best_only=True)\n",
        "model.fit(X, y, batch_size=128, epochs=3, callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "mTpJmyV2g5ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71b446c-c094-480c-ba3f-a37d796ca09d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 314ms/step - loss: 2.7586\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/model_checkpoint.py:209: UserWarning: Can save best model only with val_loss available, skipping.\n",
            "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 315ms/step - loss: 2.1116\n",
            "Epoch 3/3\n",
            "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 317ms/step - loss: 1.9382\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7805df681110>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # Import the Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical #Import to_categorical for one-hot encoding\n",
        "\n",
        "def generate_text(model, tokenizer, seed_text, max_sequence_length, temperature=1.0, num_chars=100):\n",
        "    \"\"\"\n",
        "    Generates text from a trained model.\n",
        "\n",
        "    Parameters:\n",
        "        model: Trained LSTM model\n",
        "        tokenizer: Tokenizer used during training (to convert text to tokens)\n",
        "        seed_text: Initial text to start the generation\n",
        "        max_sequence_length: Maximum sequence length used in training\n",
        "        temperature: Sampling temperature (lower is more deterministic, higher is more random)\n",
        "        num_chars: Number of characters to generate\n",
        "\n",
        "    Returns:\n",
        "        Generated text\n",
        "    \"\"\"\n",
        "    generated_text = seed_text\n",
        "    # Tokenize and pad the seed text\n",
        "    input_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    input_sequence = np.pad(input_sequence, (max_sequence_length - len(input_sequence), 0), mode='constant')\n",
        "\n",
        "    # Reshape and one-hot encode the input sequence using the correct vocabulary size\n",
        "    input_sequence = input_sequence.reshape(1, max_sequence_length)\n",
        "    input_sequence = to_categorical(input_sequence, num_classes=len(tokenizer.word_index) + 1) # Using correct vocabulary size\n",
        "\n",
        "    # Generate characters one by one\n",
        "    for _ in range(num_chars):\n",
        "        prediction_probs = model.predict(input_sequence)\n",
        "\n",
        "        # Apply temperature to prediction probabilities\n",
        "        prediction_probs = prediction_probs[:, -1, :]  # Focus on the last predicted character\n",
        "        prediction_probs = np.asarray(prediction_probs).flatten()\n",
        "        prediction_probs = np.log(prediction_probs + 1e-7) / temperature\n",
        "        prediction_probs = np.exp(prediction_probs) / np.sum(np.exp(prediction_probs))\n",
        "\n",
        "        # Sample a character from the predicted probabilities, ensuring it's within the vocabulary\n",
        "        predicted_index = np.random.choice(range(len(prediction_probs)), p=prediction_probs)\n",
        "\n",
        "        # Check if predicted_index is within the vocabulary\n",
        "        if predicted_index in tokenizer.index_word:\n",
        "            predicted_char = tokenizer.index_word[predicted_index]\n",
        "        else:\n",
        "            # Handle the case where the predicted index is out of vocabulary\n",
        "            # You could use a special token like \"<UNK>\" or choose the most frequent character\n",
        "            predicted_char = tokenizer.index_word[1]  # Example: using the most frequent character\n",
        "\n",
        "        # Append predicted character to the generated text\n",
        "        generated_text += predicted_char\n",
        "\n",
        "        # Update the input sequence with the new character\n",
        "        input_sequence = np.roll(input_sequence, shift=-1, axis=1)\n",
        "        input_sequence[0, -1] = to_categorical(predicted_index, num_classes=len(tokenizer.word_index) + 1)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "UuarSlLHv03e"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.history.history"
      ],
      "metadata": {
        "id": "7igAI7bouETZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the training history\n",
        "with open('training_history.pkl', 'wb') as f:\n",
        "    pickle.dump(history, f)"
      ],
      "metadata": {
        "id": "-aSRO0mPt78S"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.keys())"
      ],
      "metadata": {
        "id": "lM1kpSkdvLH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the training and validation loss over epochs\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history['loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Print the final loss values\n",
        "final_train_loss = history['loss'][-1]\n",
        "final_val_loss = history['val_loss'][-1]\n",
        "\n",
        "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "ZVuqyK5Suji2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}